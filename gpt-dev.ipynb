{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maleckicoa/micro-gpt/blob/main/gpt-dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load Data"
      ],
      "metadata": {
        "id": "hprLxaxjGrYe"
      },
      "id": "hprLxaxjGrYe"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "\n",
        "with open('input.txt','r', encoding='utf-8') as f:\n",
        "  text=f.read()\n",
        "\n",
        "print('lenght of dataset:', len(text))\n",
        "print(text[:200])\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "print(''.join(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "650Q0jzYecLZ",
        "outputId": "2c4c73cf-5b1d-49e3-9dae-ed61c6f8700e"
      },
      "id": "650Q0jzYecLZ",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-07 17:07:01--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-11-07 17:07:01 (30.2 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n",
            "lenght of dataset: 1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n",
            "65\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define Encode & Decode functions\n",
        "- Train Test Split"
      ],
      "metadata": {
        "id": "dROwxlqVGu5-"
      },
      "id": "dROwxlqVGu5-"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "#print(itos)\n",
        "#print(encode(\"hii there\"))\n",
        "#print(decode(encode(\"hii there\n",
        "#print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr69ODOpfSUt",
        "outputId": "4f4cee3f-0939-448b-e0bd-ec31f8d1e33d"
      },
      "id": "xr69ODOpfSUt",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Split sequences in blocks"
      ],
      "metadata": {
        "id": "AgxyEZO_G5sT"
      },
      "id": "AgxyEZO_G5sT"
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]\n",
        "##########################\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3FWduaRj3Z3",
        "outputId": "2c081647-d2f9-4de7-adc4-031782fa13c2"
      },
      "id": "X3FWduaRj3Z3",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Set up the function for fetching batches"
      ],
      "metadata": {
        "id": "Yh7ihTMDHDck"
      },
      "id": "Yh7ihTMDHDck"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data= train_data if split == 'train' else val_data\n",
        "\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "  return x,y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    #print(f\"When input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M5mGIGFlqJD",
        "outputId": "292162ef-a6dd-40a4-abcd-202ce85c4ba3"
      },
      "id": "5M5mGIGFlqJD",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Set up the Bigram NN"
      ],
      "metadata": {
        "id": "0EsfeoU3HMmP"
      },
      "id": "0EsfeoU3HMmP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Each token directly predicts the next token’s logits\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is (Batch, Time) of token indices\n",
        "        logits = self.token_embedding_table(idx)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Flatten both tensors to compute cross-entropy loss\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      # idx is (B, T) array of indices in the current context\n",
        "      for _ in range(max_new_tokens):\n",
        "          # get the predictions\n",
        "          logits, loss = self(idx)\n",
        "          # focus only on the last time step\n",
        "          logits = logits[:, -1, :] # becomes (B, C)\n",
        "          # apply softmax to get probabilities\n",
        "          probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "          # sample from the distribution\n",
        "          idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "          # append sampled index to the running sequence\n",
        "          idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "      return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nykfVJ0bqZSf",
        "outputId": "ea16e0b9-c034-4a20-cd92-bb8f563a4299"
      },
      "id": "nykfVJ0bqZSf",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
            "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Learn the model"
      ],
      "metadata": {
        "id": "nFRDgmXgHRle"
      },
      "id": "nFRDgmXgHRle"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx_IvatIx6N4",
        "outputId": "f2813c41-e13b-438f-bc03-36194abdddba"
      },
      "id": "Tx_IvatIx6N4",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.362441062927246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- See results"
      ],
      "metadata": {
        "id": "kqixJUf1HV9s"
      },
      "id": "kqixJUf1HV9s"
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kiIoycLFlt6",
        "outputId": "62f4ec35-117f-4c65-e580-cba4689ebb4d"
      },
      "id": "5kiIoycLFlt6",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "M:\n",
            "IUSh t,\n",
            "F th he d ke alved.\n",
            "Thupld, cipbll t\n",
            "I: ir w, l me sie hend lor ito'l an e\n",
            "\n",
            "I:\n",
            "Gochosen e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Self Attention"
      ],
      "metadata": {
        "id": "EpkHGT4_LivM"
      },
      "id": "EpkHGT4_LivM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1\n",
        "torch.manual_seed (1337)\n",
        "B,T,C = 4,8,2\n",
        "\n",
        "x= torch.randn(B,T,C)\n",
        "x.shape\n",
        "print(x)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CXbsSsyLmKb",
        "outputId": "62b41171-0da6-4210-800b-fc745262a56e"
      },
      "id": "3CXbsSsyLmKb",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]\n",
        "    xbow[b,t] = torch.mean(xprev,0)\n",
        "\n",
        "xbow"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kXqSzs8P5n_",
        "outputId": "7a4ad267-54eb-4dad-9970-fb604f8a27ce"
      },
      "id": "1kXqSzs8P5n_",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3\n",
        "wei = torch.tril (torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x\n",
        "torch.allclose(xbow, xbow2, atol=1e-6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu8bvUN6QdHR",
        "outputId": "b116c629-da04-458e-ac74-95cf939bff65"
      },
      "id": "Hu8bvUN6QdHR",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 4: self-attention\n",
        "\n",
        "torch.manual_seed (1337)\n",
        "B,T,C = 4,8,32\n",
        "x= torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B,T,16)\n",
        "q = query(x) # (B,T,16)\n",
        "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) ---> (B,T,T)\n",
        "\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) ##### =>> THIS MAKES IT A DECODER BLOCK\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape\n",
        "\n",
        "#wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGQ8ZMNJR2qk",
        "outputId": "a6da93cb-b3bd-47b9-8541-43bfda120891"
      },
      "id": "bGQ8ZMNJR2qk",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      if x.ndim == 2:\n",
        "        dim = 1.     # if you change for 0 to 1 -> Normalize rows instead normalizing columns\n",
        "      elif x.ndim == 3:\n",
        "        dim = (1,1)  # if you change for (0,1) to (1,1) -> Normalize rows instead normalizing columns\n",
        "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
        "      xvar = x.var(dim, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100)\n",
        "x= module(x)\n",
        "x.shape\n",
        "\n",
        "\n",
        "print(x[:,0].mean(), x[:,0].std() )\n",
        "print(x[0,:].mean(), x[0,:].std() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzVF675neH2J",
        "outputId": "05175dda-791b-4b44-df29-d6ceab64f6c1"
      },
      "id": "XzVF675neH2J",
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1469) tensor(0.8803)\n",
            "tensor(-9.5367e-09) tensor(1.0000)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}